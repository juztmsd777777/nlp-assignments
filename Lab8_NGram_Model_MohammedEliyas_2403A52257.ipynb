{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGYr3aBUaGrCbgLDGHe7Ek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juztmsd777777/nlp-assignments/blob/main/Lab8_NGram_Model_MohammedEliyas_2403A52257.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus"
      ],
      "metadata": {
        "id": "_pFBiEAFX0iS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRVL6z7QN0x7"
      },
      "outputs": [],
      "source": [
        "d1=\"I am a student in SR\"\n",
        "d2=\"I stay in the hostel\"\n",
        "d3=\"MY branch is cse\"\n",
        "d4=\"I am interested in ai\"\n",
        "d5=\"My favourite hobby is Leetcode\"\n",
        "d6=\"I like going to the library\"\n",
        "d7=\"I watch football\"\n",
        "d8=\"My favourite team is Barcelona\"\n",
        "d9=\"I like listening to all kind of music\"\n",
        "d10=\"I have delusional self belief\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unigram Counts\n"
      ],
      "metadata": {
        "id": "pfZY-OP9QONh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "\n",
        "documents = [d1, d2, d3, d4, d5, d6, d7, d8, d9, d10]\n",
        "all_text = \" \".join(documents)\n",
        "\n",
        "# Tokenize the text, convert to lowercase, and remove non-alphanumeric characters\n",
        "words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
        "\n",
        "# Generate unigram counts\n",
        "unigram_counts = collections.Counter(words)\n",
        "\n",
        "# Calculate vocabulary size\n",
        "vocabulary_size = len(unigram_counts)\n",
        "\n",
        "print(\"Unigram Counts:\")\n",
        "for word, count in unigram_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\nVocabulary Size: {vocabulary_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKUt2iaEOgFm",
        "outputId": "edf750ab-77e0-4945-bfef-6a75f2a118ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Counts:\n",
            "i: 7\n",
            "am: 2\n",
            "a: 1\n",
            "student: 1\n",
            "in: 3\n",
            "sr: 1\n",
            "stay: 1\n",
            "the: 2\n",
            "hostel: 1\n",
            "my: 3\n",
            "branch: 1\n",
            "is: 3\n",
            "cse: 1\n",
            "interested: 1\n",
            "ai: 1\n",
            "favourite: 2\n",
            "hobby: 1\n",
            "leetcode: 1\n",
            "like: 2\n",
            "going: 1\n",
            "to: 2\n",
            "library: 1\n",
            "watch: 1\n",
            "football: 1\n",
            "team: 1\n",
            "barcelona: 1\n",
            "listening: 1\n",
            "all: 1\n",
            "kind: 1\n",
            "of: 1\n",
            "music: 1\n",
            "have: 1\n",
            "delusional: 1\n",
            "self: 1\n",
            "belief: 1\n",
            "\n",
            "Vocabulary Size: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram Counts"
      ],
      "metadata": {
        "id": "CpIaobLaQSRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import bigrams\n",
        "import collections\n",
        "\n",
        "# Generate bigrams from the 'words' list\n",
        "bigram_list = list(bigrams(words))\n",
        "\n",
        "# Generate bigram counts\n",
        "bigram_counts = collections.Counter(bigram_list)\n",
        "\n",
        "\n",
        "print(\"\\nBigram Counts:\")\n",
        "for bigram, count in bigram_counts.items():\n",
        "    print(f\"{bigram}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30_todvuPm2b",
        "outputId": "f2e5164d-bf7b-4ef9-9775-c80cbdb300e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Counts:\n",
            "('i', 'am'): 2\n",
            "('am', 'a'): 1\n",
            "('a', 'student'): 1\n",
            "('student', 'in'): 1\n",
            "('in', 'sr'): 1\n",
            "('sr', 'i'): 1\n",
            "('i', 'stay'): 1\n",
            "('stay', 'in'): 1\n",
            "('in', 'the'): 1\n",
            "('the', 'hostel'): 1\n",
            "('hostel', 'my'): 1\n",
            "('my', 'branch'): 1\n",
            "('branch', 'is'): 1\n",
            "('is', 'cse'): 1\n",
            "('cse', 'i'): 1\n",
            "('am', 'interested'): 1\n",
            "('interested', 'in'): 1\n",
            "('in', 'ai'): 1\n",
            "('ai', 'my'): 1\n",
            "('my', 'favourite'): 2\n",
            "('favourite', 'hobby'): 1\n",
            "('hobby', 'is'): 1\n",
            "('is', 'leetcode'): 1\n",
            "('leetcode', 'i'): 1\n",
            "('i', 'like'): 2\n",
            "('like', 'going'): 1\n",
            "('going', 'to'): 1\n",
            "('to', 'the'): 1\n",
            "('the', 'library'): 1\n",
            "('library', 'i'): 1\n",
            "('i', 'watch'): 1\n",
            "('watch', 'football'): 1\n",
            "('football', 'my'): 1\n",
            "('favourite', 'team'): 1\n",
            "('team', 'is'): 1\n",
            "('is', 'barcelona'): 1\n",
            "('barcelona', 'i'): 1\n",
            "('like', 'listening'): 1\n",
            "('listening', 'to'): 1\n",
            "('to', 'all'): 1\n",
            "('all', 'kind'): 1\n",
            "('kind', 'of'): 1\n",
            "('of', 'music'): 1\n",
            "('music', 'i'): 1\n",
            "('i', 'have'): 1\n",
            "('have', 'delusional'): 1\n",
            "('delusional', 'self'): 1\n",
            "('self', 'belief'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trigram Counts"
      ],
      "metadata": {
        "id": "3U-ldLBoQWzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import trigrams\n",
        "import collections\n",
        "\n",
        "# Generate trigrams from the 'words' list\n",
        "trigram_list = list(trigrams(words))\n",
        "\n",
        "# Generate trigram counts\n",
        "trigram_counts = collections.Counter(trigram_list)\n",
        "\n",
        "# Calculate trigram vocabulary size\n",
        "trigram_vocabulary_size = len(trigram_counts)\n",
        "\n",
        "print(\"\\nTrigram Counts:\")\n",
        "for trigram, count in trigram_counts.items():\n",
        "    print(f\"{trigram}: {count}\")\n",
        "\n",
        "print(f\"\\nTrigram Vocabulary Size: {trigram_vocabulary_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UEjMuT2QFBg",
        "outputId": "fd75e21d-bc71-48cb-c242-2e02d08229bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trigram Counts:\n",
            "('i', 'am', 'a'): 1\n",
            "('am', 'a', 'student'): 1\n",
            "('a', 'student', 'in'): 1\n",
            "('student', 'in', 'sr'): 1\n",
            "('in', 'sr', 'i'): 1\n",
            "('sr', 'i', 'stay'): 1\n",
            "('i', 'stay', 'in'): 1\n",
            "('stay', 'in', 'the'): 1\n",
            "('in', 'the', 'hostel'): 1\n",
            "('the', 'hostel', 'my'): 1\n",
            "('hostel', 'my', 'branch'): 1\n",
            "('my', 'branch', 'is'): 1\n",
            "('branch', 'is', 'cse'): 1\n",
            "('is', 'cse', 'i'): 1\n",
            "('cse', 'i', 'am'): 1\n",
            "('i', 'am', 'interested'): 1\n",
            "('am', 'interested', 'in'): 1\n",
            "('interested', 'in', 'ai'): 1\n",
            "('in', 'ai', 'my'): 1\n",
            "('ai', 'my', 'favourite'): 1\n",
            "('my', 'favourite', 'hobby'): 1\n",
            "('favourite', 'hobby', 'is'): 1\n",
            "('hobby', 'is', 'leetcode'): 1\n",
            "('is', 'leetcode', 'i'): 1\n",
            "('leetcode', 'i', 'like'): 1\n",
            "('i', 'like', 'going'): 1\n",
            "('like', 'going', 'to'): 1\n",
            "('going', 'to', 'the'): 1\n",
            "('to', 'the', 'library'): 1\n",
            "('the', 'library', 'i'): 1\n",
            "('library', 'i', 'watch'): 1\n",
            "('i', 'watch', 'football'): 1\n",
            "('watch', 'football', 'my'): 1\n",
            "('football', 'my', 'favourite'): 1\n",
            "('my', 'favourite', 'team'): 1\n",
            "('favourite', 'team', 'is'): 1\n",
            "('team', 'is', 'barcelona'): 1\n",
            "('is', 'barcelona', 'i'): 1\n",
            "('barcelona', 'i', 'like'): 1\n",
            "('i', 'like', 'listening'): 1\n",
            "('like', 'listening', 'to'): 1\n",
            "('listening', 'to', 'all'): 1\n",
            "('to', 'all', 'kind'): 1\n",
            "('all', 'kind', 'of'): 1\n",
            "('kind', 'of', 'music'): 1\n",
            "('of', 'music', 'i'): 1\n",
            "('music', 'i', 'have'): 1\n",
            "('i', 'have', 'delusional'): 1\n",
            "('have', 'delusional', 'self'): 1\n",
            "('delusional', 'self', 'belief'): 1\n",
            "\n",
            "Trigram Vocabulary Size: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram Model"
      ],
      "metadata": {
        "id": "M-LhcV8YX5on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import bigrams\n",
        "import collections\n",
        "\n",
        "bigram_list = list(bigrams(words))\n",
        "bigram_counts = collections.Counter(bigram_list)\n",
        "bigram_vocabulary_size = len(bigram_counts)"
      ],
      "metadata": {
        "id": "v_upbcJPWvMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "TQ-OGHgmX81g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_sequence = input(\"Enter word sequence: \")\n",
        "\n",
        "w1 = word_sequence.lower().split()[-1]\n",
        "\n",
        "potential_next_words = {w2: count for (a, w2), count in bigram_counts.items() if a == w1}\n",
        "\n",
        "if not potential_next_words:\n",
        "    print(f\"No bigram found starting with '{w1}'.\")\n",
        "else:\n",
        "    w1_count = unigram_counts.get(w1, 0)\n",
        "    predicted_word = max(potential_next_words, key=lambda w2: potential_next_words[w2] / w1_count)\n",
        "    probability = potential_next_words[predicted_word] / w1_count\n",
        "    print(f\"Predicted next word: '{predicted_word}' with P('{predicted_word}' | '{w1}') = {probability:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3FTxfjjWwky",
        "outputId": "de5d237b-44c9-4621-fbe0-6b7ed0ea0846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter word sequence: i like\n",
            "Predicted next word: 'going' with P('going' | 'like') = 0.500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram with Laplace smoothening"
      ],
      "metadata": {
        "id": "7DvIGcLoYG_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import bigrams\n",
        "import collections\n",
        "\n",
        "bigram_list = list(bigrams(words))\n",
        "bigram_counts = collections.Counter(bigram_list)\n",
        "bigram_vocabulary_size = len(bigram_counts)"
      ],
      "metadata": {
        "id": "0JQ3F9kiW_Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "nNqPJQr4YOyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_sequence = input(\"Enter word sequence: \")\n",
        "\n",
        "w1 = word_sequence.lower().split()[-1]\n",
        "\n",
        "potential_next_words = {w2: count for (a, w2), count in bigram_counts.items() if a == w1}\n",
        "\n",
        "if not potential_next_words:\n",
        "    print(f\"No bigram found starting with '{w1}'.\")\n",
        "else:\n",
        "    w1_count = unigram_counts.get(w1, 0)\n",
        "    predicted_word = max(potential_next_words, key=lambda w2: (potential_next_words[w2] + 1) / (w1_count + vocabulary_size))\n",
        "    probability = (potential_next_words[predicted_word] + 1) / (w1_count + vocabulary_size)\n",
        "    print(f\"Predicted next word: '{predicted_word}' with P('{predicted_word}' | '{w1}') = {probability:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4FFoV4TXC8L",
        "outputId": "8a768413-66f4-4b2b-9729-359ee6a1176c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter word sequence: i am \n",
            "Predicted next word: 'a' with P('a' | 'am') = 0.054054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram with ADD K smoothening"
      ],
      "metadata": {
        "id": "ho0V2RMFYSRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import bigrams\n",
        "import collections\n",
        "\n",
        "bigram_list = list(bigrams(words))\n",
        "bigram_counts = collections.Counter(bigram_list)\n",
        "bigram_vocabulary_size = len(bigram_counts)"
      ],
      "metadata": {
        "id": "yK0xYufLXNNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "T3THsG5VYYjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_sequence = input(\"Enter word sequence: \")\n",
        "k = float(input(\"Enter k value: \"))\n",
        "\n",
        "w1 = word_sequence.lower().split()[-1]\n",
        "\n",
        "potential_next_words = {w2: count for (a, w2), count in bigram_counts.items() if a == w1}\n",
        "\n",
        "if not potential_next_words:\n",
        "    print(f\"No bigram found starting with '{w1}'.\")\n",
        "else:\n",
        "    w1_count = unigram_counts.get(w1, 0)\n",
        "    predicted_word = max(potential_next_words, key=lambda w2: (potential_next_words[w2] + k) / (w1_count + k * vocabulary_size))\n",
        "    probability = (potential_next_words[predicted_word] + k) / (w1_count + k * vocabulary_size)\n",
        "    print(f\"Predicted next word: '{predicted_word}' with P('{predicted_word}' | '{w1}') = {probability:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM7GM5qwXOmD",
        "outputId": "f591cef7-4810-40bb-b812-3decbf48659b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter word sequence: hemanth is\n",
            "Enter k value: 0.5\n",
            "Predicted next word: 'cse' with P('cse' | 'is') = 0.073171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trigram Model\n"
      ],
      "metadata": {
        "id": "O3Zyg-Dda7o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import trigrams\n",
        "import collections\n",
        "\n",
        "trigram_list = list(trigrams(words))\n",
        "trigram_counts = collections.Counter(trigram_list)\n",
        "trigram_vocabulary_size = len(trigram_counts)"
      ],
      "metadata": {
        "id": "P92eH-78XaTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "YvweOY8Idniw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_sequence = input(\"Enter word sequence (at least 2 words): \")\n",
        "\n",
        "sequence_words = word_sequence.lower().split()\n",
        "w1, w2 = sequence_words[-2], sequence_words[-1]\n",
        "\n",
        "potential_next_words = {w3: count for (a, b, w3), count in trigram_counts.items() if a == w1 and b == w2}\n",
        "\n",
        "if not potential_next_words:\n",
        "    print(f\"No trigram found starting with '{w1} {w2}'.\")\n",
        "else:\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    predicted_word = max(potential_next_words, key=lambda w3: potential_next_words[w3] / bigram_count)\n",
        "    probability = potential_next_words[predicted_word] / bigram_count\n",
        "    print(f\"Predicted next word: '{predicted_word}' with P('{predicted_word}' | '{w1} {w2}') = {probability:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-pV93ogXbr_",
        "outputId": "1fc76dc9-fcc4-49f6-ab61-43dba948c115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter word sequence (at least 2 words): i watch\n",
            "Predicted next word: 'football' with P('football' | 'i watch') = 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trigram with Laplace smoothening"
      ],
      "metadata": {
        "id": "s-7Vtyn3dwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_sequence = input(\"Enter word sequence (at least 2 words): \")\n",
        "\n",
        "sequence_words = word_sequence.lower().split()\n",
        "w1, w2 = sequence_words[-2], sequence_words[-1]\n",
        "\n",
        "potential_next_words = {w3: count for (a, b, w3), count in trigram_counts.items() if a == w1 and b == w2}\n",
        "\n",
        "if not potential_next_words:\n",
        "    print(f\"No trigram found starting with '{w1} {w2}'.\")\n",
        "else:\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    predicted_word = max(potential_next_words, key=lambda w3: (potential_next_words[w3] + 1) / (bigram_count + vocabulary_size))\n",
        "    probability = (potential_next_words[predicted_word] + 1) / (bigram_count + vocabulary_size)\n",
        "    print(f\"Predicted next word: '{predicted_word}' with P('{predicted_word}' | '{w1} {w2}') = {probability:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoT7xq1aXdDD",
        "outputId": "01db586c-7443-47ec-9694-499ccbda16e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter word sequence (at least 2 words): i am\n",
            "Predicted next word: 'a' with P('a' | 'i am') = 0.054054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trigram with ADD K Smoothening"
      ],
      "metadata": {
        "id": "o_qzGFV5d1EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_sequence = input(\"Enter word sequence (at least 2 words): \")\n",
        "k = float(input(\"Enter k value: \"))\n",
        "\n",
        "sequence_words = word_sequence.lower().split()\n",
        "w1, w2 = sequence_words[-2], sequence_words[-1]\n",
        "\n",
        "potential_next_words = {w3: count for (a, b, w3), count in trigram_counts.items() if a == w1 and b == w2}\n",
        "\n",
        "if not potential_next_words:\n",
        "    print(f\"No trigram found starting with '{w1} {w2}'.\")\n",
        "else:\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    predicted_word = max(potential_next_words, key=lambda w3: (potential_next_words[w3] + k) / (bigram_count + k * vocabulary_size))\n",
        "    probability = (potential_next_words[predicted_word] + k) / (bigram_count + k * vocabulary_size)\n",
        "    print(f\"Predicted next word: '{predicted_word}' with P('{predicted_word}' | '{w1} {w2}') = {probability:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MCxJnWGXegr",
        "outputId": "6d994250-0cce-441e-8080-b65c1d92cd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter word sequence (at least 2 words): is barcelona\n",
            "Enter k value: 0.04\n",
            "Predicted next word: 'i' with P('i' | 'is barcelona') = 0.433333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank You\n",
        "\n"
      ],
      "metadata": {
        "id": "nsmBUH7GZlk5"
      }
    }
  ]
}